{"themeConfig":{"themeName":"notes","postPageSize":10,"archivesPageSize":50,"siteName":"王安琪的数据分析项目","siteDescription":"初级DA，请多多指教😆😆","footerInfo":"","showFeatureImage":true,"domain":"https://Angie-DA.github.io","postUrlFormat":"SHORT_ID","tagUrlFormat":"SHORT_ID","dateFormat":"YYYY-MM-DD","feedFullText":false,"feedCount":10,"archivesPath":"archives","postPath":"","tagPath":""},"posts":[{"content":"调用包 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import json import matplotlib from pyecharts.globals import CurrentConfig, NotebookType from pyecharts import options as opts from pyecharts.charts import * from pyecharts.globals import ThemeType CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_NOTEBOOK 数据读取 # 数据量过大，这里只选取前100w行 # 根据官网介绍给定列名,具有可读性 columns=[&quot;userid&quot;, 'itemid', 'categoryid', 'type', 'timestamp'] data = pd.read_csv('E:/UserBehavior.csv', engine='python', encoding='utf-8', names=columns, chunksize=1000000, iterator=True) plt.rcParams['font.sans-serif'] = 'SimHei' # plt.rcParams['axes.unicode_minus'] = False # plt.style.use('ggplot') dataframe = data.get_chunk(1000000) 查看数据的基本信息 dataframe.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 userid 1000000 non-null int64 1 itemid 1000000 non-null int64 2 categoryid 1000000 non-null int64 3 type 1000000 non-null object 4 timestamp 1000000 non-null int64 dtypes: int64(4), object(1) memory usage: 38.1+ MB 数据清洗 dataframe.isnull().sum() userid 0 itemid 0 categoryid 0 type 0 timestamp 0 dtype: int64 查看数据没有空值的数据的个数 dataframe.nunique() userid 9739 itemid 399114 categoryid 5796 type 4 timestamp 499049 dtype: int64 查看数据的前n条 dataframe.head(n=11) userid itemid categoryid type timestamp 0 1 2268318 2520377 pv 1511544070 1 1 2333346 2520771 pv 1511561733 2 1 2576651 149192 pv 1511572885 3 1 3830808 4181361 pv 1511593493 4 1 4365585 2520377 pv 1511596146 5 1 4606018 2735466 pv 1511616481 6 1 230380 411153 pv 1511644942 7 1 3827899 2920476 pv 1511713473 8 1 3745169 2891509 pv 1511725471 9 1 1531036 2920476 pv 1511733732 10 1 2266567 4145813 pv 1511741471 转换类型、截取数据、重新排列 # 删除重复值，结果一样数据无重复有效数据 dataframe.drop_duplicates(subset=['userid','itemid','type','timestamp'],keep='first',inplace=True) dataframe.nunique() userid 9739 itemid 399114 categoryid 5796 type 4 timestamp 499049 dtype: int64 # 将时间戳转为日期，从原数据的int64转化为日期类型 import datetime # to_datetime的默认时区不是中国需要改成北京时间+8h dataframe['time'] = pd.to_datetime(dataframe['timestamp'],unit='s')+datetime.timedelta(hours = 8) dataframe.head(n=5) dataframe.info() # 只截取2017.11.25-2017.12.3期间的数据 startTime = datetime.datetime.strptime(&quot;2017-11-25 00:00:00&quot;,&quot;%Y-%m-%d %H:%M:%S&quot;) endTime = datetime.datetime.strptime(&quot;2017-12-3 23:59:59&quot;,&quot;%Y-%m-%d %H:%M:%S&quot;) # print(type(startTime)) dataframe = dataframe[(dataframe.time&gt;=startTime)&amp;(dataframe.time&lt;=endTime)] # 删除原索引重新排列新的索引 dataframe = dataframe.reset_index(drop=True) # 拆分日期和小时 dataframe['date']=dataframe.time.dt.date dataframe['hour']=dataframe.time.dt.hour #删除原数据列的时间戳属性 dataframe.drop('timestamp',inplace=True, axis=1) &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 1000000 entries, 0 to 999999 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 userid 1000000 non-null int64 1 itemid 1000000 non-null int64 2 categoryid 1000000 non-null int64 3 type 1000000 non-null object 4 timestamp 1000000 non-null int64 5 time 1000000 non-null datetime64[ns] dtypes: datetime64[ns](1), int64(4), object(1) memory usage: 53.4+ MB dataframe.head() userid itemid categoryid type time date hour 0 1 2268318 2520377 pv 2017-11-25 01:21:10 2017-11-25 1 1 1 2333346 2520771 pv 2017-11-25 06:15:33 2017-11-25 6 2 1 2576651 149192 pv 2017-11-25 09:21:25 2017-11-25 9 3 1 3830808 4181361 pv 2017-11-25 15:04:53 2017-11-25 15 4 1 4365585 2520377 pv 2017-11-25 15:49:06 2017-11-25 15 # 描述性统计数据，科学计数法表示 dataframe.describe() userid itemid categoryid hour count 9.995300e+05 9.995300e+05 9.995300e+05 999530.000000 mean 4.299721e+05 2.577625e+06 2.706119e+06 14.898745 std 4.353251e+05 1.488870e+06 1.464049e+06 6.117604 min 1.000000e+00 7.200000e+01 2.171000e+03 0.000000 25% 1.091610e+05 1.292370e+06 1.343555e+06 11.000000 50% 1.216220e+05 2.575767e+06 2.693696e+06 16.000000 75% 1.005419e+06 3.861783e+06 4.145813e+06 20.000000 max 1.018011e+06 5.163057e+06 5.161669e+06 23.000000 数据清洗结果 print('数据清洗后共有{}行记录，{}列字段，字段分别为{}'.format(dataframe.shape[0],dataframe.shape[1],dataframe.columns.tolist())) 数据清洗后共有999530行记录，7列字段，字段分别为['userid', 'itemid', 'categoryid', 'type', 'time', 'date', 'hour'] 用户数据分析 # 有效时间内总用户数量，有操作的商品数量及类目 # 独立访客数UV total_unique_users = dataframe.userid.nunique() # 有操作的商品 total_unique_itemid = dataframe.itemid.nunique() # 有操作的商品类目 total_unique_categoryid = dataframe.categoryid.nunique() # 付费用户数 user_bought_count = dataframe[dataframe['type']=='buy'].userid.nunique() # 非付费用户数 user_nobought_count = total_unique_users - user_bought_count 输出当前数据各栏目计数 total={'UV':[total_unique_users],'商品数':[total_unique_itemid],'类目数':[total_unique_categoryid],'付费用户数':[user_bought_count],'非付费用户数':[user_nobought_count]} totalnew= pd.DataFrame(total,index = ['']) totalnew UV 商品数 类目数 付费用户数 非付费用户数 9739 398971 5793 6689 3050 用户复购率 用户复购率=购买2次及以上用户数/总购买用户数 groupby_userid = dataframe.groupby('userid') user_type = groupby_userid.type.value_counts().unstack() # 使用unstack进行不堆叠操作，列方向上的索引转成行方向的索引 user_type.tail() type buy cart fav pv userid 1017990 NaN 3.0 NaN 56.0 1017994 NaN 3.0 3.0 29.0 1017997 2.0 NaN 3.0 90.0 1018000 NaN 4.0 NaN 181.0 1018011 1.0 NaN NaN 41.0 可视化图表 # 提取数据 type_series = dataframe.type.value_counts() 饼图-用户行为占比 plt.figure(figsize = (5,5), dpi = 100) plt.pie(type_series, labels=type_series.index, autopct='%1.2f%%', pctdistance = 0.8, counterclock = False, wedgeprops = {'width':0.4}) plt.title('用户行为占比') plt.show() 动态曲线-每日pv、uv #提取数据 pv_day = dataframe[dataframe.type =='pv'].groupby('date')['type'].count() uv_day = dataframe[dataframe.type =='pv'].drop_duplicates(['userid','date']).groupby('date')['userid'].count() #转换成图表所需的格式（list） #1、日期（list.index） date = pv_day.index #2、pv、uv（list.values） pv = np.around(pv_day.values/10000,decimals=2) # uv = np.around(uv_day.values,decimals=2) uv=uv_day # 制作曲线图表 x=list(date) y1=pv y2=uv pvuv_day_line = (Line(init_opts=opts.InitOpts(theme=ThemeType.DARK)) #主题设置 .add_xaxis(x) #x轴数据源 .add_yaxis('pv',#图例名字 y1, #y1轴数据源 label_opts=opts.LabelOpts(is_show=False), #不显示数据标签 ) .add_yaxis('uv',#图例名字 yaxis_index=1, #Y的双轴1号索引（区别于y1轴） y_axis=y2, #y2轴数据源 label_opts=opts.LabelOpts(is_show=False) #不显示数据标签 ) .extend_axis( #y2的轴设置 yaxis=opts.AxisOpts( name='uv',#轴名字 min_=5000,#轴起点值 max_=15000, #轴最大值 interval=1000, #轴区间间隔 axislabel_opts=opts.LabelOpts(formatter=&quot;{value} 人&quot;) #轴数据标签格式设置 ) ) .set_global_opts( #全局设置 tooltip_opts=opts.TooltipOpts(is_show=True,trigger=&quot;axis&quot;,axis_pointer_type='cross'), #随鼠标位置显示xy轴的数据、聚焦形式(交叉) xaxis_opts=opts.AxisOpts(type_='category',axispointer_opts=opts.AxisPointerOpts(is_show=True,type_=&quot;shadow&quot;)),#随鼠标位置凸显x轴长条、凸显形式（阴影） yaxis_opts=opts.AxisOpts(name='pv',axislabel_opts=opts.LabelOpts(formatter=&quot;{value} 万次&quot;)),#y1轴(默认轴)名字、轴数据标签格式设置 title_opts=opts.TitleOpts(title=&quot;每日pv和uv&quot;) #标题 ) ) pvuv_day_line.render_notebook() #展示图表 漏斗图-用户行为 第一层：用户全部行为（pv、buy、cart、fav） 第二层：收藏/加购行为（cart/fav） 第三层：付费行为（buy） 目的：分析用户行为的转化率 用户各行为数据计算方式 # 有效日期9天内各个行为的操作总数，每日平均操作数，每日平均操作用户数记录 type_df=pd.DataFrame([type_series,type_series/9,type_series/total_unique_users], index=['total','avg_day','avg_user']) # 付费用户行为记录 type_df.loc['paying_user']=dataframe[dataframe['userid'].isin(dataframe[dataframe['type']=='buy']['userid'])].type.value_counts() type_df pv cart fav buy total 895636.000000 55447.000000 28088.000000 20359.000000 avg_day 99515.111111 6160.777778 3120.888889 2262.111111 avg_user 91.963857 5.693295 2.884074 2.090461 paying_user 673147.000000 43072.000000 20373.000000 20359.000000 # 提取type用户行为列中的每一个行为数据 pv_df = dataframe[dataframe['type']=='pv'] buy_df = dataframe[dataframe['type']=='buy'] cart_df = dataframe[dataframe['type']=='cart'] fav_df = dataframe[dataframe['type']=='fav'] # 方式1 点击-&gt;加购-&gt;购买 pv_cart_df = pd.merge(left=pv_df,right=cart_df,how='inner',on=['userid','itemid','categoryid'],suffixes=('_pv','_cart')) cart_buy_df = pd.merge(left=cart_df,right=buy_df,how='inner',on=['userid','itemid','categoryid'],suffixes=('_cart','_buy')) count_users_pv_cart = pv_cart_df[pv_cart_df.time_pv &lt; pv_cart_df.time_cart].userid.nunique() count_users_cart_buy = cart_buy_df[cart_buy_df.time_cart &lt; cart_buy_df.time_buy].userid.nunique() # 方式2 点击-&gt;收藏-&gt;购买 pv_fav_df = pd.merge(left=pv_df,right=fav_df,how='inner',on=['userid','itemid','categoryid'],suffixes=('_pv','_fav')) fav_buy_df = pd.merge(left=fav_df,right=buy_df,how='inner',on=['userid','itemid','categoryid'],suffixes=('_fav','_buy')) count_user_pv_fav = pv_fav_df[pv_fav_df.time_pv &lt; pv_fav_df.time_fav].userid.nunique() count_user_fav_buy = fav_buy_df[fav_buy_df.time_fav &lt; fav_buy_df.time_buy].userid.nunique() # 方式3 点击-&gt;购买 pv_beh_buy_df = pd.merge(left=pv_df,right=buy_df,how='inner',on=['userid','itemid','categoryid'],suffixes=('_pv','_buy')) count_user_pv_beh_buy = pv_beh_buy_df[pv_beh_buy_df.time_pv &lt; pv_beh_buy_df.time_buy].userid.nunique() count_user_pv_buy = count_user_pv_beh_buy - count_users_cart_buy - count_user_fav_buy print(count_users_cart_buy) print(count_user_fav_buy) print(count_user_pv_buy) 2263 831 1573 fav_cart_ratio = (count_user_pv_fav+count_users_pv_cart)/total_unique_users buy_beh_ratio=(count_user_fav_buy+count_users_cart_buy)/total_unique_users buy_ratio = count_user_pv_buy /total_unique_users print('收藏加购用户转化率为:%.2f%%'%(fav_cart_ratio*100)) print('收藏加购-&gt;购买用户转化率为:%.2f%%'%(buy_beh_ratio*100)) print('浏览直接购买用户转化率为:%.2f%%'%(buy_ratio*100)) 收藏加购用户转化率为:58.36% 收藏加购-&gt;购买用户转化率为:31.77% 浏览直接购买用户转化率为:16.15% from pyecharts.charts import Funnel # 漏斗图datapair process_data_pair = [(&quot;点击量&quot;, total_unique_users), (&quot;收藏加购量&quot;, count_user_pv_fav+count_users_pv_cart), (&quot;购买量&quot;, count_user_fav_buy+count_users_cart_buy)] ( Funnel(init_opts=opts.InitOpts(width='500px', height='400px')) .add(&quot;type&quot;, data_pair=process_data_pair, label_opts=opts.LabelOpts(position=&quot;top&quot;), gap=2, tooltip_opts=opts.TooltipOpts(is_show=True)) .set_global_opts(title_opts=opts.TitleOpts(title=&quot;用户转化率&quot;, subtitle=&quot;process 浏览-&gt;收藏/加购-&gt;购买&quot;)) ).render_notebook() process_data_pair = [(&quot;点击量&quot;, total_unique_users), (&quot;购买量&quot;, count_user_pv_buy)] ( Funnel(init_opts=opts.InitOpts(width='500px', height='400px')) .add(&quot;type&quot;, data_pair=process_data_pair, label_opts=opts.LabelOpts(position=&quot;top&quot;), gap=2, tooltip_opts=opts.TooltipOpts(is_show=True)) .set_global_opts(title_opts=opts.TitleOpts(title=&quot;用户转化率&quot;, subtitle=&quot;process 浏览-&gt;购买&quot;)) ).render_notebook() 柱状图-用户复购率 repurchase_users = user_type[user_type['buy']&gt;=2].shape[0] # 购买记录超过1次的用户数 repurchase_rate = repurchase_users/user_bought_count print(&quot;复购率:{:.2f}%&quot;.format(repurchase_rate*100)) # 进一步分析复购次数 sns.histplot(user_type[user_type['buy']&gt;=2]['buy']-1) plt.show() 复购率:66.21% 结论：淘宝用户复购率能达到66.01%，但是进一步查看用户复购次数，发现绝大多数用户复购次数很少； 推测：淘宝的市场份额一直稳居前列，用户量足够庞大，足够的用户量复购一次也能提高复购率。 个人思考：在淘宝下沉扩展获取新用户的同时，促进老用户复购次数增加，进一步稳定提高复购率应是更重要的事情。购买一次可能是被吸睛的标题、精美的图片或是诱人的营销活动吸引，而复购就要求产品质量过关、服务到位，消费者对第一次的购物体验很满意才会进行第二次,所以淘宝可以重点在这些方面帮助商家。 动态柱状-用户量变化趋势 1、每日的活跃用户量基本在71%左右，而成交客户基本维持在20% 2、2017/12/02和2017/12/03购买量和购买人数是上涨的，而活跃用户比例较前三天下降了一个百分点，且付费用户占比，ARPU、ARPPU均下降了一到两个百分点。 3、这9天内，用户活跃天数呈现正态分布，用户活跃天数主要集中在3-7天（用户当天操作记录达3条算作当天活跃） 当日UV、当日平均访问量，当日交易用户数、交易用户比例(当日交易用户数/当日UV)，当日ARPU(总交易量/当日UVor活跃用户数)、当日ARPPU(总交易量/当日交易用户数or活跃交易用户量)，当日活跃用户数(自定义“活跃”)；——用户量变化趋势图 behavior_types=list(dict(dataframe['type'].value_counts()).keys()) behavior_types ['pv', 'cart', 'fav', 'buy'] groupby_date = dataframe.groupby(by=dataframe.date) # 根据日期分组 dates = sorted(list(dict(dataframe['date'].value_counts()).keys())) dates_df = pd.DataFrame(data=None, index=dates, columns=behavior_types) # 创建按日期分组的数据框 for d in dates: dates_df.loc[d] = groupby_date.get_group(d).type.value_counts() # 按日期填充数据框 # 日期转换为星期，datetime.datetime.isoweekday（）返回的1-7代表周一--周日 dates_df['weekday'] = [datetime.datetime.isoweekday(datetime.date(x.year,x.month,x.day)) for x in dates_df.index] dates_df pv cart fav buy weekday 2017-11-25 93932 5786 2716 1974 6 2017-11-26 95657 5784 3113 2022 7 2017-11-27 87243 5463 2873 2229 1 2017-11-28 88637 5516 2726 2220 2 2017-11-29 91334 5550 3057 2299 3 2017-11-30 94735 5638 2891 2323 4 2017-12-01 98138 6102 2999 2151 5 2017-12-02 123514 7829 3895 2536 6 2017-12-03 122446 7779 3818 2605 7 dates_df['uv'] = [groupby_date.get_group(d).userid.nunique() for d in dates] # 每日独立访客量 # 自定义当天使用淘宝app进行5次操作（无论是查看详情页、收藏、加入购物车还是购买都算）的用户就是活跃用户 active_user_standard = 3 # 自定义活跃用户操作次数基准 dates_df['dau'] = [(groupby_date.get_group(d).groupby(by='userid').size()&gt;active_user_standard).value_counts()[True] for d in dates] dates_df['au_rate'] = dates_df['dau']/dates_df['uv'] # 活跃用户比例 dates_df['buyer'] = dataframe[dataframe['type']=='buy'].groupby(by=['date','userid']).size().count(level=0) dates_df['buyer_rate'] = dates_df['buyer']/dates_df['uv'] # 付费用户比例 dates_df['ARPU'] = dates_df['buy']/dates_df['uv'] # 人均购买量=总交易量/当日UV dates_df['ARPPU'] = dates_df['buy']/dates_df['buyer'] # 付费用户人均购买量 dates_df pv cart fav buy weekday uv dau au_rate buyer buyer_rate ARPU ARPPU 2017-11-25 93932 5786 2716 1974 6 6976 5029 0.720900 1317 0.188790 0.28297 1.498861 2017-11-26 95657 5784 3113 2022 7 7128 5143 0.721521 1324 0.185746 0.28367 1.52719 2017-11-27 87243 5463 2873 2229 1 7026 5053 0.719186 1415 0.201395 0.31725 1.575265 2017-11-28 88637 5516 2726 2220 2 7032 4965 0.706058 1410 0.200512 0.3157 1.574468 2017-11-29 91334 5550 3057 2299 3 7141 5207 0.729170 1467 0.205433 0.321944 1.567144 2017-11-30 94735 5638 2891 2323 4 7243 5226 0.721524 1476 0.203783 0.320723 1.573848 2017-12-01 98138 6102 2999 2151 5 7270 5327 0.732737 1418 0.195048 0.295873 1.516925 2017-12-02 123514 7829 3895 2536 6 9567 6811 0.711926 1726 0.180412 0.265078 1.469293 2017-12-03 122446 7779 3818 2605 7 9561 6774 0.708503 1759 0.183977 0.272461 1.480955 ( Line(init_opts=opts.InitOpts(width=&quot;800px&quot;, height=&quot;500px&quot;)) .add_xaxis(xaxis_data=dates_df.index) .add_yaxis(&quot;uv&quot;, y_axis=dates_df['uv'],is_symbol_show=False) .add_yaxis(&quot;dau&quot;,y_axis=dates_df['dau'],is_symbol_show=False) .add_yaxis(&quot;buyer&quot;,y_axis=dates_df['buyer'],is_symbol_show=False) .set_global_opts( title_opts=opts.TitleOpts(title=&quot;用户量变化趋势&quot;, subtitle=&quot;2017/11/25-2017/12/03期间用户量变化趋势&quot;), tooltip_opts=opts.TooltipOpts(trigger='axis'), toolbox_opts=opts.ToolboxOpts(is_show=True), xaxis_opts=opts.AxisOpts(type_='category', boundary_gap=False)) ).render_notebook() 用户价值分析——RFM分析（Recency，Frequency，Monetary） RFM模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱3项指标来描述该客户的价值状况。 R（Recency）：客户最近一次交易时间的间隔。R值越大，表示客户交易发生的日期越久，反之则表示客户交易发生的日期越近。 F（Frequency）：客户在最近一段时间内交易的次数。F值越大，表示客户交易越频繁，反之则表示客户交易不够活跃。 M（Monetary）：客户在最近一段时间内交易的金额。M值越大，表示客户价值越高，反之则表示客户价值越低。 本次分析数据集不含金额，因此仅从R、F两个维度进行分析 # R:Recency最近一次交易日期间隔；F:Frequency频率，交易次数 groupby_buy_userid = dataframe[dataframe['type']=='buy'].groupby(by='userid') # 先创建一个空的DataFrame RF = pd.DataFrame(index=groupby_buy_userid.groups.keys(), columns=['R', 'F']) RF['F'] = groupby_buy_userid.type.count() RF['last_buy_time'] = groupby_buy_userid.time.max() # 假设获取数据集的第二天进行分析，所以选择2017/12/04为对照日期，保留天数 RF['R'] = (pd.to_datetime('2017-12-04')-RF['last_buy_time']).dt.days # 最近的一次交易记录距离2017/12/04为0天 # 即12/03购买相差还不到一天，最远的为9天，平均为2.53天 # R划分4个区域 0-1,2-3,4-6,7-9,分别得分4，3，2，1 # 交易次数最少为1次最多为93次,平均3次 # 这里的3次分母是所有购买过商品的用户 # 而之前得出的人均购买次数2次分母是所有用户 # F划分4个区域 1,2,3,4+，分别得分1，2，3，4 RF[['R','F']].describe() R F count 6689.000000 6689.000000 mean 2.524144 3.043654 std 2.397553 3.250811 min 0.000000 1.000000 25% 0.000000 1.000000 50% 2.000000 2.000000 75% 4.000000 4.000000 max 8.000000 72.000000 def R_score(x): if 0 &lt;= x &lt;= 1: return 4 elif 2 &lt;= x &lt;= 3: return 3 elif 4 &lt;= x &lt;= 6: return 2 elif 7 &lt;= x &lt;= 9: return 1 else: return 0 def F_score(x): if x == 1: return 1 elif x == 2: return 2 elif x == 3: return 3 elif x &gt;= 4: return 4 else: return 0 # 根据R,F进行评分 RF['R_score'] = RF.R.map(R_score) RF['F_score'] = RF.F.map(F_score) RF['R&gt;mean?']=(RF['R_score']&gt;RF['R_score'].mean())*1 RF['F&gt;mean?']=(RF['F_score']&gt;RF['F_score'].mean())*1 def user_classfication(tup): R_score, F_score = tup if R_score == 0 and F_score == 1: return &quot;重要保持客户&quot; elif R_score == 1 and F_score == 0: return &quot;重要发展客户&quot; elif R_score == 1 and F_score == 1: return &quot;重要价值客户&quot; elif R_score == 0 and F_score == 0: return &quot;重要挽留客户&quot; else: return None RF['user_classification'] = RF[['R&gt;mean?','F&gt;mean?']].apply(user_classfication, axis=1) RF.head() R F last_buy_time R_score F_score R&gt;mean? F&gt;mean? user_classification 100 5 8 2017-11-28 09:51:17 2 4 0 1 重要保持客户 117 5 10 2017-11-28 16:09:39 2 4 0 1 重要保持客户 119 4 3 2017-11-29 20:43:31 2 3 0 1 重要保持客户 121 8 1 2017-11-25 22:17:35 1 1 0 0 重要挽留客户 122 1 3 2017-12-02 13:59:07 4 3 1 1 重要价值客户 # 统计用户分类情况 RF.user_classification.value_counts() 重要挽留客户 2465 重要价值客户 1667 重要发展客户 1369 重要保持客户 1188 Name: user_classification, dtype: int64 # 用户分类占比 RF.user_classification.value_counts(1) 重要挽留客户 0.368515 重要价值客户 0.249215 重要发展客户 0.204664 重要保持客户 0.177605 Name: user_classification, dtype: float64 根据RFM模型对用户分类后发现重要挽留客户比例最高，其次是重要价值用户，淘宝作为成熟的电子商务平台，重要价值客户达到了一定的比例，在其他电商平台相继发展的大环境下，淘宝应该注重重要挽留客户，但是淘宝目前是中国最受欢迎的电商平台，目前只要保证口碑，适当营销并推出一系列的活动就能稳固部分重要挽留客户，同时，需要提高社群管理的效率，将重要保持客户和重要发展客户转变为重要价值客户，可采取定期发送文案、赠送优惠券等方式。要抓住用户渴望被认同、提升优越感、爱占便宜等心理进行考虑。 箱型图-用户消费习惯分析 # 用户习惯消费时间点分析 data_user_buy=dataframe[dataframe.type=='buy'] from pyecharts import options as opts from pyecharts.charts import Boxplot df = pd.DataFrame(data_user_buy['hour']) df.plot.box(title=&quot;用户下单时间&quot;) plt.grid(linestyle=&quot;--&quot;) plt.show() 个人思考：对产生过购买行为的用户分析消费时间可知，下单次数最多最密集的时间点分布在11-19点之间，中午和下午用户最习惯产生消费。为了促进消费量，商家可以在此时间点开始促销活动。 商品数据分析 条形图-热门商品分析 分别对点击量、收藏量、加购量和购买量前十的商品大类进行分析。 matplotlib.use('Qt5Agg') import matplotlib.pyplot as plt groupby_itemid = dataframe.groupby(by='itemid') # 按商品分组 item_type_df = groupby_itemid.type.value_counts().unstack() # 展开为一个dataframe item_type_df.replace(to_replace=np.nan, value=0, inplace=True) # 缺失值补0 item_pv_df = item_type_df.sort_values(by='pv', ascending=False) # 按浏览量降序排列 item_cart_df = item_type_df.sort_values(by='cart', ascending=False) # 按加购量降序排列 item_fav_df = item_type_df.sort_values(by='fav', ascending=False) # 按收藏量降序排列 item_buy_df = item_type_df.sort_values(by='buy', ascending=False) # 按购买量降序排列 item_pv_df = item_pv_df.head(10) plt.barh(range(10),item_pv_df['pv'], tick_label=item_pv_df.index) plt.ylabel('itemid') plt.xlabel('pv') plt.title('点击量前十的商品') plt.show() item_cart_df = item_cart_df.head(10) plt.barh(range(10),item_cart_df['cart'], tick_label=item_cart_df.index) plt.ylabel('itemid') plt.xlabel('cart') plt.title('加购量前十的商品') plt.show() item_fav_df = item_fav_df.head(10) plt.barh(range(10),item_fav_df['fav'], tick_label=item_fav_df.index) plt.ylabel('itemid') plt.xlabel('fav') plt.title('收藏量前十的商品') plt.show() item_buy_df = item_buy_df.head(10) plt.barh(range(10),item_buy_df['buy'], tick_label=item_buy_df.index) plt.ylabel('itemid') plt.xlabel('buy') plt.title('购买量前十的商品') plt.show() 韦恩图-热门商品分析 #添加pyvenn路径 import sys sys.path.append(r'path\\pyvenn-master') import venn labels = venn.get_labels([set(item_pv_df.index), set(item_cart_df.index), set(item_fav_df.index), set(item_buy_df.index)], fill=['number', 'percent'] ) fig, ax = venn.venn4(labels, names=['pv','cart','fav','buy']) plt.title('韦恩图') fig.show() 个人思考：根据条形图可以看出点击量、收藏量、加购量和购买量前几名较后几名差距不大，说明没有爆款的出现。 根据四者韦恩图可以看出，购买量前十品类与其他三者的重合度却没有那么高，说明对于部分品类商品而言，虽然能吸引许多用户，但是购买转化率却相对较低，这部分产品的转化率有提升空间。 ","tags":[],"title":"Python基于淘宝用户行为分析","feature":"","link":"https://Angie-DA.github.io/ePyyxEIp4/","stats":{"text":"25 min read","time":1441000,"words":5002,"minutes":25},"date":"2022-03-20 14:52:40","dateFormat":"2022-03-20"},{"content":" ","tags":[],"title":"Tableau实现连锁门店日营业情况数据监控","feature":"","link":"https://Angie-DA.github.io/9Dtl7YqLP/","stats":{"text":"0 min read","time":0,"words":0,"minutes":0},"date":"2022-03-15 15:15:34","dateFormat":"2022-03-15"}]}